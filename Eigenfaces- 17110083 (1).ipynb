{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import os\n",
    "import glob\n",
    "import matplotlib.image as mpimg\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#getting labels of images\n",
    "directory=r'D:/books/YaleFaceDatabase/'\n",
    "labels=os.listdir(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#Some data preprocessing - removing the emotion associated with every subject eg subject01.sleepy to subject01\n",
    "for i in range(len(labels)):\n",
    "    s=labels[i]\n",
    "    j=s.find('.')\n",
    "    s=s[:j]\n",
    "    labels[i]=s\n",
    "#labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#getting the filenames again\n",
    "names=os.listdir(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#getting paths for all images to load them.\n",
    "\n",
    "for i in range(len(names)):\n",
    "    s=names[i]\n",
    "    s=directory+s\n",
    "    names[i]=s\n",
    "#names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below function does the following steps:\n",
    "1. Read the image of shape (243,320).\n",
    "2. Flatten the image to vector of length 77760.\n",
    "3. Construct the train_images matrix and test_images matrix of 115 and 50 images each.\n",
    "4. It also returns the train and test labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#function to get the starting matrices\n",
    "def get_start_matrices(names,labels):\n",
    "    train,test,train_label,test_label=train_test_split(names,labels,train_size=0.7,shuffle=True)\n",
    "    train_images=[]\n",
    "    test_images=[]\n",
    "    for im in train:\n",
    "        train_images.append(np.array(Image.open(im).convert(\"L\")))\n",
    "    for im in test:\n",
    "        test_images.append(np.array(Image.open(im).convert(\"L\")))\n",
    "    train_images=np.array(train_images)\n",
    "    train_images=np.array([x.flatten() for x in train_images])\n",
    "    test_images=np.array(test_images)\n",
    "    test_images=np.array([x.flatten() for x in test_images])\n",
    "    return train_images,test_images, train_label,test_label   \n",
    "    #print(train[0],train_label[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below function does following steps:\n",
    "1. Let A be the train matrix of shape (115,77760). It computes the mean across every row (axis=1) and subtracts the mean from the matrix. We use the broadcasting feature of python to subtract the mean vector from the train matrix.\n",
    "2. We compute the matrix L as $A^{T}$ $A$ instead of $A$ $A^{T}$. This method is described by the authors themselves in another paper of theirs - Turk, M., & Pentland, A. (1991). Eigenfaces for recognition. Journal of cognitive neuroscience, 3(1), 71-86. This method helps reduce the calculations by giving us a matrix of size (115,115) rather than using the other matrix of size (77760,77760). \n",
    "3. We then find the Eigen values E_val and eigen vectors E_vec of the matrix L. The eigenvectors E_vec_U of C = $A$ $A^{T}$ are given as A* E_vec as described by the authors.\n",
    "4. Then we normalise the eigenvectors and sort them as per the eigenvalues. We sort as np.argsort(-E_val) as argsort will sort the values in ascending order, but we need the order descending. So we put in a negative sign so that the negative values get sorted in ascending order, so that the corresponding eigenvalues are sorted in descending order. We then arrange the corresponding eigenvectors. \n",
    "5. We get the set of 115 eigenvectors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def E_faces(train_images, train_label):\n",
    "    train_images = train_images.T\n",
    "    train_mean = train_images.mean(axis=1, keepdims=True)\n",
    "    #using python broadcasting to subtract mean\n",
    "    train_images = train_images - train_mean\n",
    "    L = np.dot((train_images).T, train_images)\n",
    "    E_val, E_vec = np.linalg.eigh(L)\n",
    "    E_vecs_U = np.matmul(train_images, E_vec)\n",
    "    norm = np.sum(E_vecs_U ** 2, axis=0)\n",
    "    norm = norm ** (1 / 2)\n",
    "    E_vecs_U = E_vecs_U / norm\n",
    "    indexes = np.argsort(-E_val)\n",
    "    E_val = E_val[indexes]\n",
    "    E_vecs_U = E_vecs_U[:, indexes]\n",
    "    return E_val, E_vec,E_vecs_U, train_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we select the k best eigenfaces V from the eigenface set. We also find the eigenface subspace using $V^{T}$ $A$. This is analogous to getting the coefficients when we represent each face in the training set as a linear combination of the k eigenfaces i.e. of shape (40,115)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def get_k_components(n_components,E_vecs_U,train_images):\n",
    "    k_vecs=k_vecs=E_vecs_U[:,:n_components]\n",
    "    weights = np.matmul(k_vecs.T, train_images.T- train_mean)\n",
    "    return k_vecs, weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we perform the predictions for the test set.\n",
    "1. Subtract the mean from the test images to get matrix T\n",
    "2. We then project the test faces onto the eigenface subspace using $V^{T}$ $T$ to get a matrix test_weights_all of the weights of shape (k,50).\n",
    "3. Then we pick the weights of a single test face using the matrix test_weight_single of shape (k,1). \n",
    "4. We subtract the test_weight from the weights of the training set in the matrix diff and square it. \n",
    "5. Then we add up the elements down every column and take square root to get the euclidean distance of the test face from every face. Then we pick the minimum distance using min and pick the corresponding predicted label using argmin. If the distance is below some threshold then we assign that label to that face and check the final accuracy after iterating for every face. The accuracy won't be consistent as the train set is obtained after random shuffling the dataset. So it might be possible that more instances of a particular subject enter the training set as compared to others, thereby skewing the eigenfaces. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def predict(test_images, test_label, train_label, weights, k_vecs, mean):\n",
    "    test_images = test_images.T- train_mean\n",
    "    test_label = test_label.T\n",
    "    test_weights_all = np.matmul(k_vecs.T, test_images)\n",
    "    correct_predictions=0\n",
    "    for i in range(0, len(test_label)):\n",
    "        test_weight_single = np.resize(test_weights_all[:, i], (test_weights_all.shape[0],1))\n",
    "        diff = (weights - test_weight_single) ** 2\n",
    "        Euc_dist = np.sum(diff ** (1/2), axis=0)\n",
    "        dist= Euc_dist.min(axis=0, keepdims=True)\n",
    "        y_pred=train_label[Euc_dist.argmin(axis=0)]\n",
    "        if dist < 100000:\n",
    "            if test_label[i] == y_pred:\n",
    "                correct_predictions+=1\n",
    "    print(\"The accuracy obtained is %f percent\" % (correct_predictions*100 / len(test_label)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_images,test_images, train_label,test_label = get_start_matrices(names,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "E_val,E_vec, E_vecs_U, train_mean=E_faces(train_images, train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_vecs,weights=get_k_components(40,E_vecs_U,train_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy obtained is 90.000000 percent\n"
     ]
    }
   ],
   "source": [
    "predict(test_images, np.array(test_label), np.array(train_label), weights, k_vecs, train_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
